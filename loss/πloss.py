import tensorflow as tf
from util.pd import ent, tent, ent2, tent2, logp, log_activ, sample, sample_uniform, logp_uniform, p_uniform, reg, kl, kl_std
import time
from const import Î³
from loss import Ï€_vars, v_vars, Ï€v_vars, fwd, fwdq1, fwdq2, fwdq1t, fwdq2t, rwd
from const import ns, na, reg_lvl, a_limit, a_scale, k, q
from loss import act
from util.rms import rms, r_rms
from util.tf_util import *

Î³_inv = const(1/(1-Î³), f32)
Î³_ = const(1-Î³, f32)
Î³Î³_ = const(Î³-Î³*Î³, f32)
Î³ = const(Î³, f32)
k = const(k, f32)
q_1 = const(q-1,f32)

if q == 1:
  if reg_lvl != 0:
    @tf.function
    def Ï€vgradvals(s, a, Å›, b, r):
      dist = fwd(s)
      e = ent(*dist)
      ke = k*e
      x = sample(*dist)
      logÏ€x = logp(x,*dist)
      lnax = log_activ(x)
      logÏ€x_activ = logÏ€x+lnax
      klogÏ€x = k*logÏ€x
      klogÏ€x_activ = k*logÏ€x_activ
      Ï€x = exp(logÏ€x)
      Ï€x_activ = exp(logÏ€x_activ)

      qf1x = fwdq1(s,activ(x))
      qf2x = fwdq2(s,activ(x))

      qfx = minimum(qf1x,qf2x)
      Ï€_loss = -ğ”¼(qfx-klogÏ€x_activ)

      next_dist = fwd(Å›)
      next_x = no_grad(sample(*next_dist))
      next_logÏ€x = logp(next_x,*next_dist)
      next_lnax = log_activ(next_x)
      next_logÏ€x_activ = next_logÏ€x+next_lnax
      next_klogÏ€x_activ = k*next_logÏ€x_activ
      next_Ï€x = exp(next_logÏ€x)
      next_Ï€x_activ = exp(next_logÏ€x_activ)
      next_e = ent(*next_dist)
      next_ke = k*next_e

      next_qf1 = fwdq1t(Å›,activ(next_x))
      next_qf2 = fwdq2t(Å›,activ(next_x))
      next_qf = minimum(next_qf1,next_qf2)

      q_target = no_grad(r - r_rms.mean + where(b, 0., Î³ * (next_qf - next_klogÏ€x_activ)))
      q1_loss = .5 * ğ”¼(sq(fwdq1(s,activ(a)) - q_target))
      q2_loss = .5 * ğ”¼(sq(fwdq2(s,activ(a)) - q_target))

      lnp_activ = logp(a,*dist)+log_activ(a)
      reg_rwd = r - k*lnp_activ

      v_loss = add_n([q1_loss, q2_loss])
      Ï€vgrad = concat([reshape(g,[-1]) for g in\
          grad(Ï€_loss, Ï€_vars)+grad(v_loss, v_vars)],-1)
      vals = stack([Ï€_loss, ğ”¼(e), q1_loss, r_rms.mean[0]])

      return Ï€vgrad, vals, reg_rwd
  else:
    @tf.function
    def Ï€vgradvals(s, a, Å›, b, r):
      dist = fwd(s)
      e = ent(*dist)
      ke = k*e
      x = sample(*dist)
      logÏ€x = logp(x,*dist)
      klogÏ€x = k*logÏ€x
      Ï€x = exp(logÏ€x)

      qf1x = fwdq1(s,activ(x))
      qf2x = fwdq2(s,activ(x))

      qfx = minimum(qf1x,qf2x)

      Ï€_loss = -ğ”¼(rwd(s,x)+qfx+ke)

      next_dist = fwd(Å›)
      next_x = no_grad(sample(*next_dist))
      next_logÏ€x = logp(next_x,*next_dist)
      next_klogÏ€x = k*next_logÏ€x

      next_qf1 = fwdq1t(Å›,activ(next_x))
      next_qf2 = fwdq2t(Å›,activ(next_x))
      next_qf = minimum(next_qf1,next_qf2)

      q_target = no_grad(where(b, 0., Î³ * (rwd(Å›,next_x)-r_rms.mean+next_qf - next_klogÏ€x)))
      q1_loss = .5 * ğ”¼(sq(fwdq1(s,activ(a)) - q_target))
      q2_loss = .5 * ğ”¼(sq(fwdq2(s,activ(a)) - q_target))

      reg_rwd = r +ke

      v_loss = add_n([q1_loss, q2_loss])
      Ï€vgrad = concat([reshape(g,[-1]) for g in\
          grad(Ï€_loss, Ï€_vars)+grad(v_loss, v_vars)],-1)
      vals = stack([Ï€_loss, ğ”¼(e), q1_loss, r_rms.mean[0]])

      return Ï€vgrad, vals, reg_rwd
else:
  if reg_lvl != 0:
    @tf.function
    def Ï€vgradvals(s, a, Å›, b, r):
      dist = fwd(s)
      e = ent(*dist)
      ke = k*e
      x = sample(*dist)
      logÏ€x = logp(x,*dist)
      lnax = log_activ(x)
      logÏ€x_activ = logÏ€x+lnax
      klogÏ€x = k*logÏ€x
      kqlogÏ€x_activ = k*(exp(q_1*logÏ€x_activ)-1)

      Ï€x = exp(logÏ€x)
      Ï€x_activ = exp(logÏ€x_activ)

      qf1x = fwdq1(s,activ(x))
      qf2x = fwdq2(s,activ(x))

      qfx = minimum(qf1x,qf2x)
      Ï€_loss = -ğ”¼(qfx-kqlogÏ€x_activ)

      next_dist = fwd(Å›)
      next_x = no_grad(sample(*next_dist))
      next_logÏ€x = logp(next_x,*next_dist)
      next_lnax = log_activ(next_x)
      next_logÏ€x_activ = next_logÏ€x+next_lnax
      next_kqlogÏ€x_activ = k*(exp(q_1*next_logÏ€x_activ)-1)
      next_Ï€x = exp(next_logÏ€x)
      next_Ï€x_activ = exp(next_logÏ€x_activ)
      next_e = ent(*next_dist)
      next_ke = k*next_e

      next_qf1 = fwdq1t(Å›,activ(next_x))
      next_qf2 = fwdq2t(Å›,activ(next_x))
      next_qf = minimum(next_qf1,next_qf2)

      q_target = no_grad(r - r_rms.mean + where(b, 0., Î³ * (next_qf - next_kqlogÏ€x_activ)))
      q1_loss = .5 * ğ”¼(sq(fwdq1(s,activ(a)) - q_target))
      q2_loss = .5 * ğ”¼(sq(fwdq2(s,activ(a)) - q_target))

      lnp_activ = logp(a,*dist)+log_activ(a)
      reg_rwd = r - k*(exp(q_1*lnp_activ)-1)

      v_loss = add_n([q1_loss, q2_loss])
      Ï€vgrad = concat([reshape(g,[-1]) for g in\
          grad(Ï€_loss, Ï€_vars)+grad(v_loss, v_vars)],-1)
      vals = stack([Ï€_loss, ğ”¼(e), q1_loss, r_rms.mean[0]])

      return Ï€vgrad, vals, reg_rwd
  else:
    @tf.function
    def Ï€vgradvals(s, a, Å›, b, r):
      dist = fwd(s)
      e = ent(*dist)
      te = tent(*dist)
      kte = k*te
      x = sample(*dist)
      logÏ€x = logp(x,*dist)
      kqlogÏ€x = k*(exp(q_1*logÏ€x)-1)
      Ï€x = exp(logÏ€x)

      qf1x = fwdq1(s,activ(x))
      qf2x = fwdq2(s,activ(x))

      qfx = minimum(qf1x,qf2x)

      Ï€_loss = -ğ”¼(rwd(s,x)+qfx+kte)

      next_dist = fwd(Å›)
      next_x = no_grad(sample(*next_dist))
      next_logÏ€x = logp(next_x,*next_dist)
      next_kqlogÏ€x = k*(exp(q_1*next_logÏ€x)-1)

      next_qf1 = fwdq1t(Å›,activ(next_x))
      next_qf2 = fwdq2t(Å›,activ(next_x))
      next_qf = minimum(next_qf1,next_qf2)

      q_target = no_grad(where(b, 0., Î³ * (rwd(Å›,next_x)-r_rms.mean+next_qf - next_kqlogÏ€x)))
      q1_loss = .5 * ğ”¼(sq(fwdq1(s,activ(a)) - q_target))
      q2_loss = .5 * ğ”¼(sq(fwdq2(s,activ(a)) - q_target))

      reg_rwd = r + kte

      v_loss = add_n([q1_loss, q2_loss])
      Ï€vgrad = concat([reshape(g,[-1]) for g in\
          grad(Ï€_loss, Ï€_vars)+grad(v_loss, v_vars)],-1)
      vals = stack([Ï€_loss, ğ”¼(e), q1_loss, r_rms.mean[0]])

      return Ï€vgrad, vals, reg_rwd

