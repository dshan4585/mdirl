import tensorflow as tf
from nn.layer import Dense, fwde, fwdf
from const import 풥, ns, na, nh, k
from util.rms import rms
from util.tf_util import *
from loss import act, fwd, _vars
from util.pd import logp, log_activ, ent, sample

class AIRL:
  intype = 'sa콑'
  def __init__(self, ):
    self. = 
    self.g_net = g_net = []
    self.h_net = h_net = []

    ng_in = ns+na

    g_net += [Dense(ng_in,nh)]
    g_net += [Dense(nh,nh)]
    g_net += [Dense(nh,1)]

    h_net += [Dense(ns,nh)]
    h_net += [Dense(nh,nh)]
    h_net += [Dense(nh,1)]

    self.vars = self._vars
    self.g_vars = self._g_vars
    self.h_vars = self._h_vars

  @tf.function
  def fwdg(self, s, a):
    return reshape(fwdf(concat([s,activ(a)],-1), *self.g_vars),[-1])

  @tf.function
  def fwdh(self, s):
    return reshape(fwde(s, *self.h_vars),[-1])

  @tf.function
  def rwd(self, *args):
    s, a, 콑 = args
    dist = fwd(s)
    log = logp(a,*dist)+log_activ(a)

    h = self.fwdh(s)
    g = self.fwdg(s, a)
    h패 = self.fwdh(콑)

    f = g + 풥 * h패 - h
    return k * f

  @tf.function
  def loss(self, s, a, 콑, sE, aE, 콑E):
    a = no_grad(sample(*fwd(s)))

    n = shape(s)[0]
    nE = shape(sE)[0]
    l = zeros(n)
    lE = ones(nE)
    l = concat([l,lE],0)
    s = concat([s,sE],0)
    a = concat([a,aE],0)
    g = self.fwdg(s,a)
    h = self.fwdh(s)
    h패 = self.fwdh(s)

    dist = fwd(s)
    log = logp(a,*dist)+log_activ(a)
    f = g + 풥 * h패 - h - log
    f,fE = split(f,2)

    loss = 2*洧댶(bce(l,f))

    u = uniform((n,1))
    si = u * s + (1-u) * sE
    ai = u * a + (1-u) * aE
    fi = self.fwdg(si,ai)
    f2i = self.fwdh(si)
    gi = concat(grad(fi,[si,ai]),-1)
    g2i = grad(f2i,[si])[0]
    gp = 洧댶(rsum(sq(gi),-1))+洧댶(rsum(sq(g2i),-1))

    return loss+5e-2*gp, f, fE

  @property
  def _vars(self):
    ret = []
    for layer in self.g_net + self.h_net:
      ret.extend(layer.vars)
    return ret

  @property
  def _g_vars(self):
    ret = []
    for layer in self.g_net:
      ret.extend(layer.vars)
    return ret

  @property
  def _h_vars(self):
    ret = []
    for layer in self.h_net:
      ret.extend(layer.vars)
    return ret