import tensorflow as tf
from nn.layer import Dense, BatchNorm, fwdf
from const import ns, na, nh
from util.rms import rms
from util.tf_util import *
from util.pd import sample
from loss import fwd

class DAC:
  intype = 'sa'
  def __init__(self):
    self.net = net = []
    net += [Dense(ns+na,nh)]
    net += [Dense(nh,nh)]
    net += [Dense(nh,1)]
    self.vars = self._vars

  @tf.function
  def fwd(self, s, a):
    return reshape(fwdf(concat([s,activ(a)],-1),*self.vars),[-1])

  @tf.function
  def rwd(self, *args):
    return self.fwd(*args)

  @tf.function
  def loss(self, sÏ€, aÏ€, sE, aE):
    nÏ€ = shape(sÏ€)[0]
    nE = shape(sE)[0]
    lÏ€ = zeros(nÏ€)
    lE = ones(nE)
    l = concat([lÏ€,lE],0)
    s = concat([sÏ€,sE],0)
    a = concat([aÏ€,aE],0)
    f = self.fwd(s,a)
    loss = 2*ğ”¼(bce(l,f))
    fÏ€,fE = split(f,2)

    u = uniform((nÏ€,1))
    si = u * sÏ€ + (1-u) * sE
    ai = u * aÏ€ + (1-u) * aE
    fi = self.fwd(si,ai)
    gi = concat(grad(fi,[si,ai]),-1)
    gp = ğ”¼(rsum(sq(gi),-1))

    return loss+1e-3*gp, softplus(fÏ€), softplus(fE)

  @property
  def _vars(self):
    ret = []
    for layer in self.net:
      ret.extend(layer.vars)
    return ret