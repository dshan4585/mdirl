import tensorflow as tf
from nn.layer import Dense, fwdd, fwde, fwdf, fwdg
from const import ns, na, nh, a_limit, u_limit, a_scale, q, k, k2, gp_coeff
from util.rms import rms
from util.tf_util import *
from loss import act, fwd, fwd, Ï€_vars
from util.pd import sample, log_activ, kl_std, logp

if q == 1.0:
  from util.pd import rs as r
  from util.pd import dfs as df
  from util.pd import kl as D
else:
  from util.pd import rt as r
  from util.pd import dft as df
  from util.pd import tkl as D

class RAIRLDiagGaussian2:
  intype = 'sa'
  def __init__(self, Ï€):
    self.Ï€ = Ï€
    self.Î¼_net = Î¼_net = []
    self.lnÏƒ_net = lnÏƒ_net = []
    self.b_net = b_net = []

    d = na

    Î¼_net += [Dense(ns,nh)]
    Î¼_net += [Dense(nh,nh)]
    Î¼_net += [Dense(nh,d)]

    lnÏƒ_net += [Dense(ns,nh)]
    lnÏƒ_net += [Dense(nh,nh)]
    lnÏƒ_net += [Dense(nh,na)]

    b_net += [Dense(ns,nh)]
    b_net += [Dense(nh,nh)]
    b_net += [Dense(nh,1)]

    self.vars = self._vars
    self.net_vars = self._net_vars

    self.Î¼_vars = self.net_vars[0]
    self.lnÏƒ_vars = self.net_vars[1]
    self.b_vars = self.net_vars[2]

  @tf.function
  def fwdr(self, s):
    n = shape(s)[0]

    Î¼ = tanh(fwdd(s, *self.Î¼_vars)/u_limit)*u_limit
    lnÏƒ = log_tanh(fwdd(s, *self.lnÏƒ_vars))
    return Î¼, lnÏƒ

  @tf.function
  def fwdb(self, s):
    return reshape(fwdf(s,*self.b_vars),[-1])

  @tf.function
  def rwd(self, s, a):
    return k*df(a, *self.fwdr(s)) + k2*self.fwdb(s)

  @tf.function
  def loss(self, sÏ€, aÏ€, sE, aE):
    aÏ€ = no_grad(sample(*fwd(sÏ€)))

    nÏ€ = shape(sÏ€)[0]
    nE = shape(sE)[0]
    lÏ€ = zeros(nÏ€)
    lE = ones(nE)
    l = concat([lÏ€,lE],0)
    s = concat([sÏ€,sE],0)
    a = concat([aÏ€,aE],0)

    dist = fwd(s)
    tdist = self.fwdr(s)
    b = self.fwdb(s)
    t = r(a, *dist)
    rwd = (df(a, *self.fwdr(s)) + no_grad(b))
    f = rwd - t
    rÏ€, rE = split(rwd,2)

    loss = 2*ğ”¼(bce(l,f))+2*ğ”¼(bce(l,b))

    u = uniform((nÏ€,1))
    si = u * sÏ€ + (1-u) * sE
    ai = u * aÏ€ + (1-u) * aE

    fi = logp(ai, *self.fwdr(si))
    f2i = self.fwdb(si)
    gi = grad(fi,[si])[0]
    g2i = grad(f2i,[si])[0]
    gp = .5*(ğ”¼(rsum(sq(gi),-1))+ğ”¼(rsum(sq(g2i),-1)))

    return loss+gp_coeff*gp, rÏ€, rE

  @property
  def _vars(self):
    ret = []
    for layer in self.Î¼_net + self.lnÏƒ_net + self.b_net:
      ret.extend(layer.vars)
    return ret

  @property
  def _net_vars(self):
    ret = []
    for net in [self.Î¼_net, self.lnÏƒ_net, self.b_net]:
      net_ret = []
      for layer in net:
        net_ret.extend(layer.vars)
      ret.append(net_ret)
    return ret


