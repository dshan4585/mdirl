import time
import os.path as osp
from util import log
import numpy as np
import tensorflow as tf
from util.opt import Adam
from const_discrete import Î±0, Î±T, na
from util.tf_util_discrete import *
from util.pd_discrete import breg, rf, pot, logp

np_f32 = np.float32
np_mean = np.mean

def train(alg, Ï€, Ï€E, D, batch_size, *,
     Ï€_step, d_step,
     lr_Ï€=1e-2, lr_d=1e-2,
     max_steps=0, expr_path='', record_intvl=10, save_intvl=-1, 
     burnin_steps=0):
  Opt = lambda x: Adam(x, Î²1=0.5, Î²2=0.99)
  DOpt = lambda x: Adam(x, Î²1=0.9, Î²2=0.99)
  opt = Opt(Ï€.vars)
  dopt = DOpt(D.vars)
  rwd = D.rwd

  @tf.function
  def tf_locals(dvals):
    return tf.concat([ð”¼(dvals, 0)], 0)

  logit = Ï€.logit
  @tf.function
  def gradvals(a):
    return grad(na *(pot(logit) - ð”¼(logp(a,logit)*rf(a,D.rlogit))),[logit])[0]

  @tf.function
  def dgradvals(*args):
    loss, rÏ€, rE = D.loss(*args)
    return concat([reshape(g,[-1]) for g in grad(na*loss,D.vars)],-1), stack([loss, ð”¼(rÏ€), ð”¼(rE)])

  steps = 0
  i = 0
  l = 0

  dvals = []
  keys = []
  if alg == 'mdirl':
    keys += ['pi/div', 'pi/rev','pi/rdiv', 'pi/rrev','pi/tdiv', 'pi/trev', 'pi/alpha']
  else:
    keys += ['pi/div', 'pi/rev','pi/rdiv', 'pi/rrev']
  keys += ['disc/loss', 'disc/r', 'disc/rE']

  if alg == 'mdirl':
    assert Î±T >= Î±0

  Î±t = const(Î±0, f32)
  train_Ï€ = False

  while True:
    if not(train_Ï€) and steps >= burnin_steps:
      train_Ï€ = True

    start_time = time.time()

    t_rate = np.maximum(steps - burnin_steps, 0) / max_steps
    Î±t = tf.constant(t_rate * (Î±T - Î±0) + Î±0, f32)

    for j in range(d_step):
      xÏ€ = Ï€.act_batch(batch_size)
      xE = Ï€E.act_batch(batch_size)
      if alg == 'mdirl':
        dgrad, dval = dgradvals(*[xÏ€, xE, Î±t])
      else:
        dgrad, dval = dgradvals(*[xÏ€, xE])
      dopt.update(dgrad, lr_d)
      dvals.append(dval)

    Ï€.logit.assign(D.rlogit)
    for j in range(Ï€_step):
      xÏ€ = Ï€.act_batch(batch_size)
      opt.update(gradvals(xÏ€), lr_Ï€)

    if save_intvl > 0 and (steps - burnin_steps) >= (l * save_intvl):
      param_dict = {}
      for idx, var in enumerate(Ï€.vars):
        param_dict[f'pi_{idx}'] = var.numpy()
      for idx, var in enumerate(Ï€E.vars):
        param_dict[f'piE_{idx}'] = var.numpy()
      if D is not None:
        for idx, var in enumerate(D.vars):
          param_dict[f'd_{idx}'] = var.numpy()
      param_path = osp.join(expr_path, f'param_{l * save_intvl}.npz')
      with open(param_path, 'wb') as f:
        np.savez(f, **param_dict)
      l += 1

    if (i%record_intvl) == 0:

      args = [dvals]
      if alg == 'mdirl':
        vals = np.concatenate([[
            breg(Ï€.logit, Ï€E.logit),breg(Ï€E.logit, Ï€.logit),
            breg(D.rlogit, Ï€E.logit),breg(Ï€E.logit, D.rlogit),
            breg(D.tlogit, Ï€E.logit),breg(Ï€E.logit, D.tlogit),Î±t],
            tf_locals(dvals)])
      else:
        vals = np.concatenate([[
            breg(Ï€.logit, Ï€E.logit),breg(Ï€E.logit, Ï€.logit),
            breg(D.rlogit, Ï€E.logit),breg(Ï€E.logit, D.rlogit),],tf_locals(dvals)])

      for key, val in zip(keys, vals):
        log.record_tab(key, val)
      log.record_tab('ep/steps', steps)
      log.dump_tab(i//record_intvl, time.time() - start_time)

      dvals.clear()

    i += 1
    steps += Ï€_step

    if max_steps and (steps - burnin_steps) >= max_steps:
      break