import time
import os.path as osp
from util import log
import numpy as np
import tensorflow as tf
from random import shuffle
from util.act import generate_Ï„, generate_Ï„2
from util.opt import Adam
from const import Î±0, Î±T, Î³, q, k
from loss import act, fwd
from util.pd import logp, ent
from util.tf_util import *
from util.rms import rms, r_rms
import gc


np_f32 = np.float32
np_mean = np.mean

if q == 1.0:
  from util.pd import rs as r
else:
  from util.pd import rt as r

def train(alg, env, eval_env, Ï€, buf, D, expert_dataset, batch_size, *,
     Ï€_step, d_step,
     lr_Ï€=3e-4, lr_d=3e-4,
     max_steps=0, expr_path='', record_intvl=10, save_intvl=-1, 
     burnin_steps=0):

  is_2d = "Multi" in env.name
  if is_2d:
    lr_Ï€ = 5e-4
    lr_d = 5e-4

  Opt = lambda x: Adam(x, Î²1=0.9, Î²2=0.999)
  DOpt = lambda x: Adam(x, Î²1=0.9, Î²2=0.999)

  from loss import Ï€v_vars
  Ï€opt = Opt(Ï€v_vars)

  do_irl = D is not None

  Ï„_sampler = generate_Ï„(Ï€, env, batch_size)
  eval_sampler = generate_Ï„(Ï€, eval_env, 4000)

  get_Ï„ = Ï„_sampler.__next__
  evaluate = eval_sampler.__next__

  get_batch = buf.get_next_batch
  if do_irl:
    get_batchÏ€ = buf.get_next_batch_adv
    get_batchE = expert_dataset.get_next_batch
    from loss import d_vars
    dopt = DOpt(d_vars)
    rwd = D.rwd
    nintype = len(D.intype)
    @tf.function
    def tf_locals(Ï€vals, dvals):
      return tf.concat([ð”¼(Ï€vals, 0), ð”¼(dvals, 0)], 0)
  else:
    @tf.function
    def tf_locals(Ï€vals):
      return ð”¼(Ï€vals, 0)

  from loss.Ï€loss import Ï€vgradvals
  if do_irl:
    from loss.dloss import dgradvals

  ep_total = 0
  steps = 0
  i = 0
  l = 0

  lr_Ï€_vec = lr_Ï€ * ones(add_n([rprod(x)
      for x in shape_n(Ï€.vars)]), f32)
  lr_q_vec = lr_d * ones(add_n([rprod(x)
      for x in shape_n(Ï€.q1_vars)]), f32)

  ep_cnt = 0
  ep_lens = []
  ep_rets = []
  r_trues = []

  Ï€vals = []
  dvals = []

  keys = ['ep/ep_cnt', 'ep/ep_len', 'ep/ep_ret', 'ep/r_true']
  if alg == 'mdirl':
    keys += ['pi/alpha']
  keys += ['pi/loss','pi/entropy',
      'value/q_loss', 'value/r_mean']
  if do_irl:
    keys += ['disc/loss', 'disc/r', 'disc/rE']

  if alg == 'mdirl':
    assert Î±T >= Î±0

  Î±t = const(Î±0, f32)
  train_Ï€ = False

  while True:
    if not(train_Ï€) and steps >= burnin_steps:
      train_Ï€ = True

    start_time = time.time()

    t_rate = np.maximum(steps - burnin_steps, 0) / max_steps
    Î±t = tf.constant(t_rate * (Î±T - Î±0) + Î±0, f32)

    if do_irl:
      for j in range(d_step):
        xÏ€ = get_batchÏ€(batch_size)
        xE = get_batchE(batch_size)
        if alg == 'mdirl':
          dgrad, dval = dgradvals(*xÏ€, *xE, Î±t)
        else:
          dgrad, dval = dgradvals(*xÏ€, *xE)

        del(xÏ€)
        del(xE)

        dopt.update(dgrad, lr_d)

        del(dgrad)
        dvals.append(dval)

    for j in range(Ï€_step):
      if steps % 100 == 0:
        Ï„ = get_Ï„()
        buf.add(batch_size, Ï„)

      x = get_batch(batch_size)

      if do_irl:
        r = rwd(*x[:nintype])
        x = x[:4]+[r]


      Ï€vgrad, Ï€vval, reg_rewards = Ï€vgradvals(*x)

      if not(train_Ï€):
        r_rms.update(reg_rewards[...,np.newaxis])
      else:
        if steps % 10000 <= 5:
          r_rms.update(reg_rewards[...,np.newaxis])

      if train_Ï€:
        Ï€opt.update(Ï€vgrad, lr_Ï€)
        del(Ï€vgrad)
      Ï€vals.append(Ï€vval)
      Ï€.updatevt(x[-1])

      del(x)

      steps += 1

      if save_intvl > 0 and (steps - burnin_steps) >= (l * save_intvl):
        param_dict = {}
        for idx, var in enumerate(Ï€.vars):
          param_dict[f'pi_{idx}'] = var.numpy()
        for idx, var in enumerate(r_rms.vars):
          param_dict[f'r_rms_{idx}'] = var.numpy()
        if D is not None:
          for idx, var in enumerate(D.vars):
            param_dict[f'd_{idx}'] = var.numpy()
        param_path = osp.join(expr_path, f'param_{l * save_intvl}.npz')
        with open(param_path, 'wb') as f:
          np.savez(f, **param_dict)
        l += 1

    if (i%(record_intvl//Ï€_step)) == 0:
      evals = evaluate()
      ep_cnt += len(evals['ep_len'])
      ep_lens.extend(evals['ep_len'])
      ep_rets.extend(evals['ep_ret'])
      r_trues.append(np_mean(evals['r_true'], dtype=np_f32))
      ep_total += ep_cnt

      if do_irl:
        args = [Ï€vals, dvals]
      else:
        args = [Ï€vals]
      if alg == 'mdirl':
        vals = np.concatenate([[
            np_f32(ep_cnt),
            np_mean(ep_lens, dtype=np_f32),
            np_mean(ep_rets, dtype=np_f32),
            np_mean(r_trues, dtype=np_f32), Î±t],
            tf_locals(*args)])
      else:
        vals = np.concatenate([[
          np_f32(ep_cnt),
          np_mean(ep_lens, dtype=np_f32),
          np_mean(ep_rets, dtype=np_f32),
          np_mean(r_trues, dtype=np_f32)],
          tf_locals(*args)])

      for key, val in zip(keys, vals):
        log.record_tab(key, val)
      log.record_tab('ep/ep_total', ep_total)
      log.record_tab('ep/steps', steps)
      log.dump_tab(i//(record_intvl//Ï€_step), time.time() - start_time)

      ep_cnt = 0
      ep_lens.clear()
      ep_rets.clear()
      r_trues.clear()

      Ï€vals.clear()
      dvals.clear()

    gc.collect()
    i += 1

    if max_steps and (steps - burnin_steps) >= max_steps:
      break
